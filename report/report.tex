\documentclass[11pt]{article}
\usepackage{geometry}
\geometry{margin=.94in}
\usepackage[OT1]{fontenc}
\usepackage{booktabs}
\usepackage{adjustbox}
%\usepackage{emoji}
\usepackage[backend=bibtex8,style=authoryear]{biblatex}
\bibliography{lib}

\title{\vspace{-1.5cm}IDS703 Final Project Report}
\author{Anna Dai, Satvik Kishore, Moritz Wilksch}
\date{December 12th, 2021}

\begin{document}
\maketitle

% --------------------------------------------------------------------------------
\section{Introduction}

In this project, we work on tweet classification as a Natural Language Processing problem, more specifically, as a document classification problem. Twitter is a microblogging service where users post publicly visible ``Tweets'', which are essentially texts with less than 280 characters. These Tweets may also contain other media objects which are discarded for the purposes of this project. These Tweets most often serve as discussion pieces as part of larger conversations. They are relevant to any number of topics under discussion. These ``topics" are also often explicitly highlighted by the user using a "hashtag", i.e. text with '\#' followed by the topic name, or a commonly used shorthand for it. For the purpose of our project, we treat these hashtags as "topics" for our document classification model, where each Tweet is an instance of a document.

% --------------------------------------------------------------------------------
\section{Data}

\subsection{Collection}
We manually select seven popular topics (hashtags) for classification: crypto, tesla, championsleague, formula1, thanksgiving, holidays, and covid19.
These topics were intentionally selected so that some topics have some degree of overlap between them (e.g. holidays and thanksgiving), others are easier to differentiate (e.g. crypto vs formula1), and is an independent topic that is often mentioned with the others (i.e. covid19). We leverage the python library twint (\cite{twint}) to scrape approximately 10,000 Tweets, or documents, for each of these seven topics.


\subsection{Pre-Processing}

The raw scraped Tweets are quite messy, as they are cluttered with links to webpages, images, mentions of other Twitter users (i.e. @usernames) and other hashtags, so we spend some time to carefully pre-process the data before proceeding with modeling. As a first step, we tokenize each document into words. The id of each unique token is stored in a encoder object. [TODO: elimination of common words?]. Conversion of emojis into tokens (each appearance of an emoji is a token. Multiple emojis strung together are treated as different tokens in sequence). We also removed punctuation marks. Following these steps, we split our data into three parts using a 60:20:20 split to form a training dataset, a validation dataset, and a testing dataset. 

The collected corpus of almost 70,000 tweets is randomly split into a training, validation, and test set in a 60/20/20 ratio. We will train models on the train set, find optimal hyperparameters using the validation set, and report all performance metrics in the \emph{Results} section on the test set, which is not used for any other purpose than evaluating each model once.



\noindent 


% --------------------------------------------------------------------------------

\section{Modeling}
\subsection{Generative Model}
We use a Latent Dirichlet Allocation (LDA) model as a generative model to learn from the corpus we have collected. This type of model does not require any hyperparamter tuning, and thus is trained using a combination of the training and validation dataset. We used the LDA implementation from scikit-learn (\cite{sklearn}). It is fit to the corpus to find seven separate topics, as this is the number of actual topics in the training set. Subsequently, for each inferred topic, we manually inspect the top 50 words that are associated with it to assign it a name. This manual step is necessary, as the order of topics is not preserved. In fact, the LDA model does not even guarantee to find the same topics that were collected in the original data set. This shortcoming will be discussed in the \emph{Results} and \emph{Conclusion} sections. 
To use the LDA for document classification,  we let it infer the topic distribution of each document in the test set and use the $argmax(\cdot)$ function to assign each document the topic that is most prevalent according to the LDA.
Finally, the LDA is used to generate synthetic data. For each topic, we sample 10,000 artificial documents, the length of which is sampled from the empirical distribution of tweet lengths each time. Similar to the actual data set, the synthetic data set is also split into a train, validation, and test set.


\subsection{Discriminative Model}
We implemented a discriminative model as a neural network model to be able to classify input documents into their respective cateogries. These documents can arise from the synthetic data generated by the LDA or the real data scraped from twitter. The model architecture we used was a bidirectional Gated Recurrent Unit (GRU) at its core. This model is able to accept inputs of varying length and outputs a vector of length 7 for each input document. This is the vector of predicted probabilities for each of the seven topics. These probabilities are generated by the softmax layer at its end of the model. This model was created and trained using Tensorflow \cite{tensorflow}


\subsubsection{Model Architecture}
\fbox{\includegraphics[width=\textwidth,height=\textheight,keepaspectratio]{architecture.drawio.png}}

The model contains three essential subparts: the embedding, the bidrectional GRU, and the final densely connected layer. Each layer had one or more hyperparameter that were subject to tuning. These hyperparameters are denoted by $HP(i)$. The model is fed the document in the form of tokens identified from the encoder. There were "X" number of unique tokens. The embedding layer is a neural net that converts each token into a vector of length $HP(0)$. These embedded vectors are then received as input by the two GRU layers. GRU are a form of recurrent neural networks that use gates to better pass long term memory but have smaller complexity than LSTM. GRU also do not differentiate between output vector and hidden vector. The size of these hidden vectors was $HP(1)$. They are faster and easier to train than LSTMs while being more expressive than simple RNNs. The final hidden vectors of both GRU (forward and backward) are then concatenated and fed into a dropout layer with dropout rate $HP(2)$ followed by a densely connected network of two layers seperated by another dropout layer with rate $HP(2)$., These dense layers were of size $HP(3)$ and 7, and had activation layers of ReLU and Softmax respectively.
- Adam optimizer
- Learning Rate found using the Learning Rate range test (= 10 ** -2.5)(\cite{smith2018disciplined})

\subsubsection{Training Process}
1) hyperparameter tuning on synth data
2) benchmark on synth and real
3) continue training on real data
4) benchmark on synth and real
5) Completely new model: Hyperparametertuning only on real data
6) benchmark on synth and real


% --------------------------------------------------------------------------------
\section{Results}
\subsection{LDA Model}

The LDA model performs exceptionally well scoring precision and recall values of around 0.98 on synthetic data. This is to be expected, as the model itself generated the data. Therefore, there is only a small amount of randomness from the sampling procedure that prevents the model from achieving an accuracy of 100\%. However, applying the same model to real-world data reveals that this performance does not translate well. The topics \texttt{thanksgiving} and \texttt{covid19} in particular seem to be hard to classify by the model as they have the lowest precision and recall values of all topics. The topic \texttt{Formula1} seems to be the easiest to classify, as around 55.8\% of the models \texttt{Formula1} predictions are correct (precision) and the model is able to identify more than half of the \texttt{Formula1} related tweets (recall = 0.562). The model achieves an accuracy of $\approx X \%$ on synthetic and $\approx X\%$ on real data.
\begin{center}
\input{./benchmark_outputs/lda_classificationreport_synthdata.tex}
\qquad
\input{./benchmark_outputs/lda_classificationreport_realdata.tex}
\end{center}
\begin{center}
Table 1 \& 2: Benchmark results of LDA model on synthetic data (left) and real data (right)
\end{center}

\subsection{Neural Networks}
\subsubsection{Training on Synthetic Data Only}
The neural network trained on synthetic data is almost able to achieve the same performance characteristics as the LDA model, although it performs slightly worse for almost every topic. The network

\begin{center}
\input{./benchmark_outputs/synth_only_model_classificationreport_synthdata.tex}	
\qquad
\input{./benchmark_outputs/synth_only_model_classificationreport_realdata.tex}
\end{center}
\begin{center}
Table 3 \& 4: Benchmark results of neural net (trained on synthetic data only) on synthetic data (left) and real data (right)
\end{center}




\subsubsection{Training on Synthetic Data and Real Data}
\begin{center}
\input{./benchmark_outputs/synthandreal_model_classificationreport_synthdata.tex}
\qquad
\input{./benchmark_outputs/synthandreal_model_classificationreport_realdata.tex}
\end{center}
\begin{center}
Table 3 \& 4: Benchmark results of neural net (trained on synthetic data 

and real data) on synthetic data (left) and real data (right)
\end{center}


\subsubsection{Training on Real Data Only}
\begin{center}
\input{./benchmark_outputs/justreal_model_classificationreport_synthdata.tex}
\qquad
\input{./benchmark_outputs/justreal_model_classificationreport_realdata.tex}
\end{center}
\begin{center}
Table 5 \& 6: Benchmark results of neural net (trained on real data only)

 on synthetic data (left) and real data (right)
\end{center}


\subsection{Learned Word Embeddings}
To verify that the neural network model has learned meaningful word embeddings, we feed it 7 manually chosen words (one for each topic, but never the name of the topic itself) and find the ten closest word in the embedding space using cosine similarity. We find that the word embeddings capture an impressive amount of meaning: The name of Formula1 driver Lewis Hamilton is closest to other driver's names as well as "formula" and "race". "Barcelona" maps to other sports clubs that played in the championsleague. "Vaccine" is close to health- and covid-related words while "Christmas" is similar to texttt{december} and "festive". For "Elon" and "BTC" the closest words are coherent and reffer to Tesla and Cryptocurrencies. Only "turkey" is not clearly close to thanksgiving, presumably as the topic "thanksgiving" has been hard to identify throughout the experiments.
TODO:
- Do the accuracies support this view?


\begin{adjustbox}{width=\columnwidth,center}
\input{embeddings_similar_words.tex}
\end{adjustbox}
\begin{center}
	Table 9: Top 10 closest words in embedding space for seven examples.
\end{center}

% --------------------------------------------------------------------------------
\section{Discussion and Conclusion}



% --------------------------------------------------------------------------------
\newpage
\printbibliography

\end{document}
