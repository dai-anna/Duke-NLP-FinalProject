\documentclass[11pt]{article}
\usepackage{geometry}
\geometry{margin=.94in}
\usepackage[OT1]{fontenc}
\usepackage{booktabs}
\usepackage{adjustbox}
%\usepackage{emoji}

\usepackage[backend=bibtex8,style=authoryear]{biblatex}
\bibliography{lib}

\title{\vspace{-1.5cm}IDS703 Final Project Report}
\author{Anna Dai, Satvik Kishore, Moritz Wilksch}
\date{December 12th, 2021}

\begin{document}
\maketitle

% --------------------------------------------------------------------------------
\section{Introduction}

In this project, we work on tweet classification as a Natural Language Processing problem, more specifically, as a document classification problem. Twitter is a microblogging service where users post publicly visible ``Tweets'', which are essentially texts with less than 280 characters. These Tweets may also contain other media objects which are discarded for the purposes of this project. These Tweets most often serve as discussion pieces as part of larger conversations. They are relevant to any number of topics under discussion. These ``topics" are also often explicitly highlighted by the user using a "hashtag", i.e. text with '\#' followed by the topic name, or a commonly used shorthand for it. For the purpose of our project, we treat these hashtags as "topics" for our document classification model, where each Tweet is an instance of a document.

% --------------------------------------------------------------------------------
\section{Data}

\subsection{Collection}
We manually select seven popular topics (hashtags) for classification: crypto, tesla, championsleague, formula1, thanksgiving, holidays, and covid19.
These topics were intentionally selected so that some topics have some degree of overlap between them (e.g. holidays and thanksgiving), others are easier to differentiate (e.g. crypto vs formula1), and is an independent topic that is often mentioned with the others (i.e. covid19). We leverage the python library twint (\cite{twint}) to scrape approximately 10,000 Tweets, or documents, for each of these seven topics.


\subsection{Pre-Processing}

The raw scraped Tweets are quite messy, as they are cluttered with links to webpages, images, mentions of other Twitter users (i.e. @usernames) and other hashtags, so we spend some time to carefully pre-process the data before proceeding with modeling. As a first step, we tokenize each document into words. [TODO: elimination of common words?]. Conversion of emojis into tokens (each appearance of an emoji is a token. Multiple emojis strung together are treated as different tokens in sequence). We also removed punctuation marks. Following these steps, we split our data into three parts using a 60:20:20 split to form a training dataset, a validation dataset, and a testing dataset. 

The collected corpus of almost 70,000 tweets is randomly split into a training, validation, and test set in a 60/20/20 ratio. We will train models on the train set, find optimal hyperparameters using the validation set, and report all performance metrics in the \emph{Results} section on the test set, which is not used for any other purpose than evaluating each model once.



\noindent 


% --------------------------------------------------------------------------------

\section{Modeling}
\subsection{Generative Model}
We use a Latent Dirichlet Allocation (LDA) model as a generative model to learn from the corpus we have collected. This type of model does not require any hyperparamter tuning, and thus is trained using a combination of the training and validation dataset. We used the LDA implementation from scikit-learn (\cite{sklearn}). It is fit to the corpus to find seven separate topics, as this is the number of actual topics in the training set. Subsequently, for each inferred topic, we manually inspect the top 50 words that are associated with it to assign it a name. This manual step is necessary, as the order of topics is not preserved. In fact, the LDA model does not even guarantee to find the same topics that were collected in the original data set. This shortcoming will be discussed in the \emph{Results} and \emph{Conclusion} sections. 
To use the LDA for document classification,  we let it infer the topic distribution of each document in the test set and use the $argmax(\cdot)$ function to assign each document the topic that is most prevalent according to the LDA.
Finally, the LDA is used to generate synthetic data. For each topic, we sample 10,000 artificial documents, the length of which is sampled from the empirical distribution of tweet lengths each time. Similar to the actual data set, the synthetic data set is also split into a train, validation, and test set.


\subsection{Discriminative Model}
\subsubsection{Model Architecture}
- network description
- Adam optimizer
- Learning Rate found using the Learning Rate range test (= 10 ** -2.5)(\cite{smith2018disciplined})


	
 	

\subsubsection{Training Process}
1) hyperparameter tuning on synth data
2) benchmark on synth and real
3) continue training on real data
4) benchmark on synth and real
5) Completely new model: Hyperparametertuning only on real data
6) benchmark on synth and real

% --------------------------------------------------------------------------------
\section{Results}
\subsection{Benchmarking on Synthetic Data}

\begin{center}
\input{./benchmark_outputs/synth_only_model_classificationreport_synthdata.tex}	
\qquad
\input{./benchmark_outputs/synth_only_model_classificationreport_realdata.tex}
\end{center}
\begin{center}
Table 1: Benchmark results of neural net (trained on synthetic data only) on synthetic data

Table 2: Benchmark results of neural net (trained on synthetic data only) on real data
\end{center}


\subsection{Benchmarking on Real Data}

\begin{center}
\input{./benchmark_outputs/synthandreal_model_classificationreport_synthdata.tex}
\end{center}
\begin{center}
Table 3: Benchmark results of neural net (trained on synthetic data and real data) on synthetic data
\end{center}


\begin{center}
\input{./benchmark_outputs/synthandreal_model_classificationreport_synthdata.tex}
\qquad
\input{./benchmark_outputs/synthandreal_model_classificationreport_realdata.tex}
\end{center}
\begin{center}
Table 3: Benchmark results of neural net (trained on synthetic data and real data) on synthetic data

Table 4: Benchmark results of neural net (trained on synthetic data and real data) on real data
\end{center}

\begin{center}
\input{./benchmark_outputs/justreal_model_classificationreport_synthdata.tex}
\qquad
\input{./benchmark_outputs/justreal_model_classificationreport_realdata.tex}
\end{center}
\begin{center}
Table 5: Benchmark results of neural net (trained on real data only) on synthetic data

Table 6: Benchmark results of neural net (trained on real data only) on real data
\end{center}

\begin{center}
\input{./benchmark_outputs/lda_classificationreport_synthdata.tex}
\qquad
\input{./benchmark_outputs/lda_classificationreport_realdata.tex}
\end{center}
\begin{center}
Table 7: Benchmark results of LDA classification on synthetic data

Table 8: Benchmark results of LDA classification on real data

\end{center}

\subsection{Learned Word Embeddings}
To verify that the neural network model has learned meaningful word embeddings, we feed it 7 manually chosen words (one for each topic, but never the name of the topic itself) and find the ten closest word in the embedding space using cosine similarity. We find that the word embeddings capture an impressive amount of meaning: The name of Formula1 driver Lewis Hamilton is closest to other driver's names as well as "formula" and "race". "Barcelona" maps to other sports clubs that played in the championsleague. "Vaccine" is close to health- and covid-related words while "Christmas" is similar to "december" and "festive". For "Elon" and "BTC" the closest words are coherent and reffer to Tesla and Cryptocurrencies. Only "turkey" is not clearly close to thanksgiving, presumably as the topic "thanksgiving" has been hard to identify throughout the experiments.
TODO:
- Do the accuracies support this view?


\begin{adjustbox}{width=\columnwidth,center}
\input{embeddings_similar_words.tex}
\end{adjustbox}
\begin{center}
	Table 9: Top 10 closest words in embedding space for seven examples.
\end{center}

% --------------------------------------------------------------------------------
\section{Discussion and Conclusion}



% --------------------------------------------------------------------------------
\newpage
\printbibliography

\end{document}
